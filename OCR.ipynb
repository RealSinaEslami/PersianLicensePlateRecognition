{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462784b5",
   "metadata": {},
   "source": [
    "#### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b48116",
   "metadata": {},
   "source": [
    "#### 2. Define vocabularies for model inputs and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {'EIN':'I', 'HE':'E', 'SAD':'A', 'SIN':'S', 'TA':'X', 'ZH':'W', 'TH':'G', 'SH':'U'}\n",
    "\n",
    "characters = {'0': 0,'1': 1,'2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,\n",
    "              'B': 10,'D': 11,'EIN': 12,'H': 13,'HE': 14,'J': 15, 'L':16, 'M':17, 'N': 18,\n",
    "             'P': 19,'Q': 20,'SAD': 21,'SIN': 22,'T': 23,'TA': 24,'V': 25,'Y': 26, 'Z': 27,\n",
    "             'SH': 28, 'TH':29, 'ZH': 30, 'A': 31, ' ': 32}\n",
    "\n",
    "en_fa = {'0': 0,'1': 1,'2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,\n",
    "         'ز': 'Z','ش': 'SH','ط': 'TA','پ': 'P','ث': 'TH','ژ (معلولین و جانبازان)': 'ZH',\n",
    "         'الف': 'A','ع': 'EIN','ه‍': 'H','ق': 'Q','ت': 'T','م': 'M','ل': 'L','د': 'D',\n",
    "         'ی': 'Y','ب': 'B', 'ج': 'J', 'ن': 'N'}\n",
    "\n",
    "indexes = {'0': 0,'1': 1,'2': 2,'3': 3,'4': 4,'5': 5,'6': 6,'7': 7,'8': 8,'9': 9,\n",
    "          'B': 10,'D': 11,'I': 12,'H': 13,'E': 14,'J': 15, 'L':16, 'M':17, 'N': 18,\n",
    "         'P': 19,'Q': 20,'A': 21,'S': 22,'T': 23,'X': 24,'V': 25,'Y': 26, 'Z': 27,\n",
    "          'U': 28,'G': 29,'W': 30,'A': 31,' ': 32}\n",
    "\n",
    "rev_indexes = {v: k for k, v in indexes.items()}\n",
    "n_classes = len(indexes) + 1  # +1 for CTC blank\n",
    "blank_index = n_classes - 1 # The last char is blank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33181a12",
   "metadata": {},
   "source": [
    "#### 3. Define the path to your data and image transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a768b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.path.join(\"OCR-Data/SelectedData/images/\")\n",
    "labels_path = os.path.join(\"OCR-Data/SelectedData/labels/\")\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.01):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05,0.1)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "    transforms.Resize((32, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.01),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52610375",
   "metadata": {},
   "source": [
    "#### 4. Loading data\n",
    "##### 4.1. Loading generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bcf118",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = \"OCR-Data/SelectedData/gen/\"\n",
    "\n",
    "for img in os.listdir(gen_path):\n",
    "    full_name = img.split('.')[0]\n",
    "    file_name = re.split(\"_|-|\\.\", img)\n",
    "    idx = file_name[0]\n",
    "    if file_name[2] in abbreviations:\n",
    "        file_name[2] = abbreviations[file_name[2]]\n",
    "    name = ''.join(file_name[1:-1])\n",
    "    with open(labels_path+full_name+'.txt', 'w') as f:\n",
    "        f.write(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261262c",
   "metadata": {},
   "source": [
    "##### 4.2. Loading real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca445070",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_path = os.path.join(\"OCR-Data/SelectedData/img/\")\n",
    "xmls_path = os.path.join(\"OCR-Data/SelectedData/xml/\")\n",
    "\n",
    "real = sorted(os.listdir(real_path))\n",
    "xmls = sorted(os.listdir(xmls_path))\n",
    "\n",
    "for xml in xmls:\n",
    "    file_name = xml.split(\".\")[0]\n",
    "    tree = ET.parse(xmls_path + xml)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    name = root.findall(\".//name\")\n",
    "    file = []\n",
    "\n",
    "    for n in name:\n",
    "        temp = en_fa[n.text]\n",
    "        if temp in abbreviations:\n",
    "            temp = abbreviations[temp]\n",
    "        file.append(str(temp))\n",
    "    with open(labels_path+file_name+'.txt', 'w') as f:\n",
    "        f.write(''.join(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f52b12",
   "metadata": {},
   "source": [
    "##### 4.3. Loading data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d47b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = sorted(os.listdir(images_path))\n",
    "labels_list = sorted(os.listdir(labels_path))\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label in labels_list:\n",
    "    with open(labels_path + label, 'r') as f:\n",
    "        idx_values = []\n",
    "        values = list(f.read())\n",
    "        for v in values:\n",
    "            idx_values.append(indexes[v])\n",
    "        labels.append(idx_values)\n",
    "        \n",
    "for img in images_list:\n",
    "    image = Image.open(images_path + img).convert('RGB')\n",
    "    image = transform(image)\n",
    "    images.append(image)\n",
    "    \n",
    "    \n",
    "for i in range(len(labels)-1, -1, -1):\n",
    "    if len(labels[i]) != 8:\n",
    "        del labels[i]\n",
    "        del images[i]\n",
    "        \n",
    "images = torch.stack(images)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c98dca",
   "metadata": {},
   "source": [
    "#### 5. Creating CRNN Model ---> 2D-CONV + LSTM + CTC-LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac15b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, nc, nclass, nh, dropout_prob=0.3):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(nc, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(True),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Conv2d(512, 512, 2, 1, 0), nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.rnn1 = nn.LSTM(512, nh, bidirectional=True)\n",
    "        self.rnn2 = nn.LSTM(nh * 2, nh, bidirectional=True)\n",
    "        self.dropout_rnn = nn.Dropout(dropout_prob)\n",
    "        self.embedding = nn.Linear(nh * 2, nclass)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, f\"Unexpected height: {h}\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)\n",
    "\n",
    "        recurrent, _ = self.rnn1(conv)\n",
    "        recurrent, _ = self.rnn2(recurrent)\n",
    "        recurrent = self.dropout_rnn(recurrent)\n",
    "        output = self.embedding(recurrent)\n",
    "        return(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16e18c",
   "metadata": {},
   "source": [
    "#### 7. Split data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d65d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, images_val, labels_train, labels_val = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d13fc",
   "metadata": {},
   "source": [
    "#### 8. Compile Model on GPU/CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN(imgH=32, nc=3, nclass=n_classes, nh=256)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "train_dataset = TensorDataset(images_train, labels_train)\n",
    "val_dataset = TensorDataset(images_val, labels_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "ctc_loss = nn.CTCLoss(blank=blank_index, zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f09def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images_batch, labels_batch in train_loader:\n",
    "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images_batch)\n",
    "        input_lengths = torch.full((labels_batch.size(0),), output.size(0), dtype=torch.long).to(device)\n",
    "        target_lengths = torch.full((labels_batch.size(0),), labels_batch.size(1), dtype=torch.long).to(device)\n",
    "        \n",
    "        loss = ctc_loss(output.log_softmax(2), labels_batch, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images_batch, labels_batch in val_loader:\n",
    "            images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
    "            output = model(images_batch)\n",
    "            input_lengths = torch.full((labels_batch.size(0),), output.size(0), dtype=torch.long).to(device)\n",
    "            target_lengths = torch.full((labels_batch.size(0),), labels_batch.size(1), dtype=torch.long).to(device)\n",
    "            loss = ctc_loss(output.log_softmax(2), labels_batch, input_lengths, target_lengths)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886a24c",
   "metadata": {},
   "source": [
    "#### 9. Decode result of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prediction(pred):\n",
    "    pred = pred.permute(1, 0, 2)\n",
    "    pred_labels = torch.argmax(pred, dim=2)\n",
    "    results = []\n",
    "    for seq in pred_labels:\n",
    "        prev = -1\n",
    "        text = ''\n",
    "        for p in seq:\n",
    "            p = p.item()\n",
    "            if p != blank_index and p != prev:\n",
    "                text += rev_indexes.get(p, '')\n",
    "            prev = p\n",
    "        results.append(text)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d655fb6",
   "metadata": {},
   "source": [
    "#### 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    image = images[1].unsqueeze(0).to(device)\n",
    "    output = model(image)\n",
    "    decoded = decode_prediction(output)\n",
    "    print(\"Predicted:\", decoded[0])\n",
    "    print(\"Ground truth:\", ''.join([rev_indexes[v.item()] for v in labels[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847ffd1",
   "metadata": {},
   "source": [
    "#### 11. Saving weights of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'OCRModel/crnn_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
